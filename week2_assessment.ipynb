{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold learning\n",
    "\n",
    "Study the Tutorial tutorial_manifold_tSNE and the tutorial_manifold_spectral_clustering and the Study_Case_pipeline. Next improve the code by comparing the performance of k-means and spectral clustering. Also compare PCA and t-SNE in the visualization of the result. You can use the pipeline function of scikit-learn and hyperparameter tuning with GridSearchCV. Here's a possible approach:\n",
    "\n",
    "- Load the dataset to be used for the clustering analysis.\n",
    "- Preprocess the dataset as needed (e.g., scale the features, normalize the data, etc.).\n",
    "- Define a pipeline with preprocessing and clustering\n",
    "- use PCA and t-SNE for dimension reduction and visualize the dimensions, use the clusters to color the datapoints\n",
    "- use GridSearchCV to optimize the hyper parameters\n",
    "- Evaluate the performance of the models using a suitable metric\n",
    "- choose the best cluster method and the best visualization method combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load data\n",
    "brain_data = pd.read_csv(\"../Data/Brain_GSE50161.csv\")\n",
    "# Drop a samples column\n",
    "brain_data = brain_data.drop('samples', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheke a uique value in type columns\n",
    "unique_value = brain_data['type'].unique()\n",
    "unique_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes = brain_data['type'].value_counts()\n",
    "# Get the unique names\n",
    "unique_names = datatypes.index\n",
    "\n",
    "unique_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for zero values\n",
    "zero_value = brain_data.eq(0).sum().sum()\n",
    "zero_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the dataset as needed "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the target\n",
    "targets =brain_data['type']\n",
    "# Remove any non-numeric columns from the dataset(extract the features)\n",
    "features = brain_data.select_dtypes(include=[float, int])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness =features .skew()\n",
    "highly_skewed_count = 0\n",
    "approximately_symmetric_count = 0\n",
    "\n",
    "# Count the number of columns in each skewness category\n",
    "for skewness_value in skewness:\n",
    "    if skewness_value < 0.75:\n",
    "        highly_skewed_count += 1\n",
    "    else:\n",
    "        approximately_symmetric_count += 1\n",
    "\n",
    "print(f\"Number of highly skewed columns: {highly_skewed_count}\")\n",
    "print(f\"Number of approximately symmetric columns: {approximately_symmetric_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform log transformation on skewed_columns\n",
    "skew_columns = (features.skew().sort_values(ascending = False))\n",
    "skew_columns = skew_columns[skew_columns > 0.75]\n",
    "for col in skew_columns.index.tolist():\n",
    "   features[col] = np.log1p(features[col])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and axes for the box plots\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "# Select the first 30 columns from your dataset\n",
    "selected_columns = features.columns[:30]\n",
    "\n",
    "# Create a box plot for the selected columns\n",
    "plt.boxplot(features[selected_columns])\n",
    "\n",
    "# Set the x-axis tick labels\n",
    "plt.xticks(range(1, len(selected_columns) + 1), selected_columns, rotation='vertical')\n",
    "\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Box Plot of First 30 Columns')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see in the plot of the first 30 columns the features are not in the same scale and we need to scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "scaled_features = sc.fit(features).transform(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualization with t-SNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### dimension reduction using t-sne ans PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_clustering(data, n_pca_components=50, n_tsne_components=2):\n",
    "     \n",
    "    \"\"\"\n",
    "    Perform PCA and t-SNE dimension reduction on the input data.\n",
    "\n",
    "    Args:\n",
    "        data: The input data to perform dimension reduction on.\n",
    "        n_pca_components (optional): The number of components to keep in the PCA step.\n",
    "            Defaults to 50 if not provided.\n",
    "        n_tsne_components (optional): The number of components to keep in the t-SNE step.\n",
    "            Defaults to 2 if not provided.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of PCA and t-SNE results: (pca_result, tsne_result).\n",
    "    \"\"\"\n",
    "        \n",
    "    # Perform PCA for dimension reduction\n",
    "    pca = PCA(n_components=n_pca_components)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    # Perform t-SNE for further dimension reduction\n",
    "    tsne = TSNE(n_components=n_tsne_components, random_state=0, method='barnes_hut')\n",
    "    tsne_result = tsne.fit_transform(data)\n",
    "\n",
    "    return pca_result, tsne_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call  function perform_clustering_clusterin\n",
    "pca_result, tsne_result = perform_clustering_clustering(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame of results\n",
    "results = pd.DataFrame({\n",
    "'PCA1': pca_result[:, 0],\n",
    "'PCA2': pca_result[:, 1],\n",
    "'t-SNE1': tsne_result[:, 0],\n",
    "'t-SNE2': tsne_result[:, 1],    \n",
    "})\n",
    "\n",
    "def plot_results(results, targets):\n",
    "    \"\"\"\n",
    "Plot the results of dimension reduction using scatter plots.\n",
    "\n",
    "Args:\n",
    "    results: The DataFrame containing the dimension reduction results (PCA and t-SNE).\n",
    "    targets: The target variable used to color the scatter plots.\n",
    "\n",
    "Returns:\n",
    "    plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x='PCA1', y='PCA2', hue=targets, data=results, palette='Set1')\n",
    "    plt.title('PCA')\n",
    "    plt.legend(title='Target', loc='best')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x='t-SNE1', y='t-SNE2', hue=targets, data=results, palette='Set1')\n",
    "    plt.title('t-SNE')\n",
    "    plt.legend(title='Target', loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "Hyperparameters are parameters that are set before the learning process begins and control the behavior of a learning algorithm. They determine the values of model parameters that the learning algorithm will learn during training. Hyperparameters are not learned from the data but are chosen by the user or determined through a search process https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization with differnt range of perplexities(hyperparameter) for T_SNE  with out appliying PCA dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_tsne_perplexity(data, targets, perplexities=[5, 10, 20, 30, 40, 50]):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot t-SNE results for different perplexity values.\n",
    "\n",
    "    Args:\n",
    "        data: The input data to perform t-SNE on.\n",
    "        targets: Target variable for coloring the scatter plots.\n",
    "        perplexities: A list of perplexity values to iterate over.\n",
    "            Defaults to [5, 10, 20, 30, 40, 50] if not provided.\n",
    "\n",
    "    Returns:\n",
    "        plot\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results = pd.DataFrame(columns=['Perplexity', 't-SNE1', 't-SNE2', 'Target'])\n",
    "\n",
    "    # Iterate over perplexity values\n",
    "    for perplexity in perplexities:\n",
    "        # Create a t-SNE instance with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "\n",
    "        # Fit and transform the features\n",
    "        tsne_result = tsne.fit_transform(data)\n",
    "\n",
    "        # Append the results to the DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Perplexity': perplexity,\n",
    "            't-SNE1': tsne_result[:, 0],\n",
    "            't-SNE2': tsne_result[:, 1],\n",
    "            'Target': targets\n",
    "        })\n",
    "        results = pd.concat([results, df], ignore_index=True)\n",
    "\n",
    "    # Plot t-SNE results for each perplexity\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, perplexity in enumerate(perplexities):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        subset = results[results['Perplexity'] == perplexity]\n",
    "        sns.scatterplot(x='t-SNE1', y='t-SNE2', data=subset, hue='Target', palette='Set1', legend=False)\n",
    "        plt.title(f\"Perplexity = {perplexity}\")\n",
    "\n",
    "    # Show a single legend label for the whole plot\n",
    "    plt.tight_layout()\n",
    "    sns.set_context('notebook')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_perplexity(scaled_features, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA and T_SNE dimension reduction and kmean clustering with differnt range of perplexities for TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kmean_tsne(data, n_pca_components=50, n_clusters=5, perplexities=[5, 10, 20, 30, 40, 50]):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform K-means clustering on PCA-transformed data and plot t-SNE results for different perplexity values.\n",
    "\n",
    "    Args:\n",
    "        data: The input data to perform K-means clustering and t-SNE on.\n",
    "        n_pca_components : The number of components to keep in the PCA step. Defaults to 50 if not provided.\n",
    "        n_clusters: The number of clusters to create using K-means clustering. Defaults to 5 if not provided.\n",
    "        perplexities: A list of perplexity values to iterate over in t-SNE. Defaults to [5, 10, 20, 30, 40, 50] if not provided.\n",
    "\n",
    "    Returns:\n",
    "        plot\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=n_pca_components)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    # Perform clustering using K-means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(pca_result)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results = pd.DataFrame(columns=['Perplexity', 't-SNE1', 't-SNE2', 'Cluster'])\n",
    "\n",
    "    # Iterate over perplexity values\n",
    "    for perplexity in perplexities:\n",
    "        # Create a t-SNE instance with the current perplexity value\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "\n",
    "        # Fit and transform the features\n",
    "        tsne_result = tsne.fit_transform(pca_result)\n",
    "\n",
    "        # Append the results to the DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Perplexity': perplexity,\n",
    "            't-SNE1': tsne_result[:, 0],\n",
    "            't-SNE2': tsne_result[:, 1],\n",
    "            'Cluster': cluster_labels\n",
    "        })\n",
    "        results = pd.concat([results, df], ignore_index=True)\n",
    "\n",
    "    # Plot t-SNE results for each perplexity\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, perplexity in enumerate(perplexities):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        subset = results[results['Perplexity'] == perplexity]\n",
    "        sns.scatterplot(x='t-SNE1', y='t-SNE2', data=subset, hue='Cluster', palette='Set1', legend=False)\n",
    "        plt.title(f\"Perplexity = {perplexity}\")\n",
    "\n",
    "    # Show a single legend label for the whole plot\n",
    "    plt.tight_layout()\n",
    "    sns.set_context('notebook')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmean_tsne(data=scaled_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA and T_SNE dimension reduction with differnt range of n_inits for kmean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_perplexity_ninit(data, n_pca_components=50, perplexities=[5, 10, 20, 30, 40, 50], n_inits=[10, 20, 30]):\n",
    "    \"\"\"\n",
    "    Perform PCA to reduce dimensionality, apply t-SNE with different perplexity and different n_init values for kmean clustring,\n",
    "    and plot the t-SNE results for each combination of perplexity and n_init.\n",
    "\n",
    "    Args:\n",
    "        data: The input data to perform PCA, t-SNE, and K-means clustering on.\n",
    "        n_pca_components: The number of components to keep in the PCA step. Defaults to 50 if not provided.\n",
    "        perplexities: A list of perplexity values to iterate over in t-SNE. Defaults to [5, 10, 20, 30, 40, 50] if not provided.\n",
    "        n_inits: A list of n_init values to iterate over in K-means clustering. Defaults to [10, 20, 30] if not provided.\n",
    "\n",
    "    Returns:\n",
    "        plot\n",
    "    \"\"\"\n",
    "    # Perform PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=n_pca_components)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results = pd.DataFrame(columns=['Perplexity', 'n_init', 't-SNE1', 't-SNE2', 'Cluster'])\n",
    "\n",
    "    # Iterate over perplexity and n_init values\n",
    "    for perplexity in perplexities:\n",
    "        for n_init in n_inits:\n",
    "            # Create a t-SNE instance with the current perplexity value\n",
    "            tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "\n",
    "            # Fit and transform the features\n",
    "            tsne_result = tsne.fit_transform(pca_result)\n",
    "\n",
    "            # Perform clustering using K-means with the current n_init value\n",
    "            kmeans = KMeans(n_clusters=5, n_init=n_init)\n",
    "            cluster_labels = kmeans.fit_predict(pca_result)\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            df = pd.DataFrame({\n",
    "                'Perplexity': perplexity,\n",
    "                'n_init': n_init,\n",
    "                't-SNE1': tsne_result[:, 0],\n",
    "                't-SNE2': tsne_result[:, 1],\n",
    "                'Cluster': cluster_labels\n",
    "            })\n",
    "            results = pd.concat([results, df], ignore_index=True)\n",
    "\n",
    "    # Plot t-SNE results for each perplexity and n_init combination\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    for i, perplexity in enumerate(perplexities):\n",
    "        for j, n_init in enumerate(n_inits):\n",
    "            plt.subplot(len(perplexities), len(n_inits), i * len(n_inits) + j + 1)\n",
    "            subset = results[(results['Perplexity'] == perplexity) & (results['n_init'] == n_init)]\n",
    "            sns.scatterplot(x='t-SNE1', y='t-SNE2', data=subset, hue='Cluster', palette='Set1', legend=False)\n",
    "            plt.title(f\"Perplexity = {perplexity}, n_init = {n_init}\")\n",
    "\n",
    "    # Show a single legend label for the whole plot\n",
    "    plt.tight_layout()\n",
    "    sns.set_context('notebook')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_perplexity_ninit(data=scaled_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best hyperparameters for the K-means clustering algorithm applied to t-SNE embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE to get embeddings\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_result = tsne.fit_transform(scaled_features)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_clusters': [3, 4, 5],\n",
    "    'n_init': [10, 20, 30]\n",
    "}\n",
    "# Instantiate KMeans object\n",
    "kmeans = KMeans()\n",
    "\n",
    "# Create GridSearchCV object with silhouette_score as the scoring metric\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=kmeans, \n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # default 3-fold cross-validation\n",
    "    n_jobs=5,\n",
    "    verbose=1 #Controls the verbosity: the higher, the more messages.\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the t-SNE embeddings\n",
    "grid_search.fit(tsne_result)\n",
    "\n",
    "# Retrieve the best hyperparameter values and corresponding score\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best number of clusters:\", best_params['n_clusters'])\n",
    "print(\"Best number of initializations:\", best_params['n_init'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line of the output indicats search is being performed with 5-fold cross-validation and a total of 9 different combinations of hyperparameters. and accourding to this result 5 numbres of cluster and 10 initalization(Number of times the k-means algorithm is run with different centroid seeds) are the best parameters.https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA and T_SNE dimension reduction with differnt range of gammas for  SpectralClustering clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_tsne_perplexity_gamma(data, n_pca_components=50, perplexities=[5, 10, 20, 30, 40, 50], gammas=[0.1, 0.5, 1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Perform PCA to reduce dimensionality, apply t-SNE with different perplexity and gamma values,\n",
    "    perform spectral clustering, and plot the t-SNE results for each combination of perplexity and gamma.\n",
    "\n",
    "    Args:\n",
    "        data: The input data to perform PCA, t-SNE, and spectral clustering on.\n",
    "        targets: The target labels for the data.\n",
    "        n_pca_components : The number of components to keep in the PCA step. Defaults to 50 if not provided.\n",
    "        perplexities : A list of perplexity values to iterate over in t-SNE. Defaults to [5, 10, 20, 30, 40, 50] if not provided.\n",
    "        gammas : A list of gamma values to iterate over in spectral clustering. Defaults to [0.1, 0.5, 1, 5, 10] if not provided.\n",
    "\n",
    "    Returns:\n",
    "        plot\n",
    "    \"\"\"\n",
    "    # Perform PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=n_pca_components)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results = pd.DataFrame(columns=['Perplexity', 'Gamma', 't-SNE1', 't-SNE2', 'spectral'])\n",
    "\n",
    "    # Iterate over perplexity and gamma values\n",
    "    for perplexity in perplexities:\n",
    "        for gamma in gammas:\n",
    "            # Create a t-SNE instance with the current perplexity value\n",
    "            tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "\n",
    "            # Fit and transform the features\n",
    "            tsne_result = tsne.fit_transform(pca_result)\n",
    "\n",
    "            # Perform spectral clustering\n",
    "            spectral = SpectralClustering(n_clusters=5, gamma=gamma)\n",
    "            cluster_labels = spectral.fit_predict(pca_result)\n",
    "\n",
    "            # Append the results to the DataFrame\n",
    "            df = pd.DataFrame({\n",
    "                'Perplexity': perplexity,\n",
    "                'Gamma': gamma,\n",
    "                't-SNE1': tsne_result[:, 0],\n",
    "                't-SNE2': tsne_result[:, 1],\n",
    "                'Target': targets\n",
    "            })\n",
    "            results = pd.concat([results, df], ignore_index=True)\n",
    "\n",
    "    # Plot t-SNE results for each perplexity and gamma combination\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    for i, perplexity in enumerate(perplexities):\n",
    "        for j, gamma in enumerate(gammas):\n",
    "            plt.subplot(len(perplexities), len(gammas), i * len(gammas) + j + 1)\n",
    "            subset = results[(results['Perplexity'] == perplexity) & (results['Gamma'] == gamma)]\n",
    "            sns.scatterplot(x='t-SNE1', y='t-SNE2', data=subset, hue='spectral', palette='Set1', legend=False)\n",
    "            plt.title(f\"Perplexity = {perplexity}, Gamma = {gamma}\")\n",
    "\n",
    "    # Show a single legend label for the whole plot\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Plot the pairplot visualization\n",
    "    sns.set_context('notebook')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_perplexity_gamma(scaled_features, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kmean and SpectralClustering Clustering Performance Evaluation Metrics\n",
    "\n",
    "#### Most popular evaluation metrics for clustering algorithms are the Silhouette coefficient, calinski_harabasz_score and davies_bouldin_score\n",
    "\n",
    "The Silhouette score measures the quality of clustering,by  calculates the average distance between the sample and all other points within the same cluster inaddition it computes the average distance between the sample and all points in the nearest neighboring cluster. values closer to +1 indicating dense and well-separated clusters, values around 0 suggesting overlapping clusters, and values closer to -1 indicating incorrect clustering.\n",
    "\n",
    "Calinski_harabasz_score calculates the ratio of the sum of between-clusters dispersion to the sum of within-cluster dispersion for all clusters. A high Calinski-Harabasz score suggests that the clusters are well-separated and distinct, with minimal overlap. \n",
    "\n",
    "davies_bouldin_score measures the average similarity between clusters by comparing the distance between clusters with the size of the clusters themselves.A lower Davies-Bouldin index indicates better separation between the clusters. https://medium.com/@haataa/how-to-measure-clustering-performances-when-there-are-no-ground-truth-db027e9a871c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(data):\n",
    "    \"\"\"\n",
    "    Perform evaluation of kmean and spectral clustering\n",
    "\n",
    "    Args:\n",
    "        data: The input data to perform PCA, t-SNE, and spectral clustering on.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Perform PCA on scaled features\n",
    "    pca = PCA(n_components=50)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    # Create a t-SNE instance with the current perplexity value\n",
    "    tsne = TSNE(n_components=2, perplexity=30)\n",
    "\n",
    "    # Fit and transform the features\n",
    "    tsne_result = tsne.fit_transform(pca_result)\n",
    "\n",
    "    # Apply k-means clustering\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "    kmeans_labels = kmeans.fit_predict(pca_result)\n",
    "\n",
    "    # Apply spectral clustering\n",
    "    spectral = SpectralClustering(n_clusters=5, random_state=42)\n",
    "    spectral_labels = spectral.fit_predict(pca_result)\n",
    "\n",
    "    # # Store the K-means and spectral clustering results in dataframes\n",
    "    # kmean_df = pd.concat([features, pd.DataFrame(kmeans_labels)], axis=1)\n",
    "    # spectral_df = pd.concat([features, pd.DataFrame(spectral_labels)], axis=1)\n",
    "\n",
    "    # # Prepare cluster lists for evaluation\n",
    "    # kmean_cluster_list = [kmean_df.values]\n",
    "    # spectral_cluster_list = [spectral_df.values]\n",
    "\n",
    "    # Evaluate clustering performance\n",
    "    kmeans_silhouette = silhouette_score(pca_result, kmeans_labels )\n",
    "    spectral_silhouette = silhouette_score(pca_result, spectral_labels)\n",
    "\n",
    "    kmeans_calinski = calinski_harabasz_score(pca_result,  kmeans_labels )\n",
    "    spectral_calinski = calinski_harabasz_score(pca_result, spectral_labels)\n",
    "\n",
    "    kmeans_davies = davies_bouldin_score(pca_result,  kmeans_labels )\n",
    "    spectral_davies = davies_bouldin_score(pca_result, spectral_labels)\n",
    "\n",
    "    # Print the evaluation results\n",
    "    print(\"K-means clustering:\")\n",
    "    print(\"Silhouette score:\", kmeans_silhouette)\n",
    "    print(\"Calinski-Harabasz score:\", kmeans_calinski)\n",
    "    print(\"Davies-Bouldin score:\", kmeans_davies)\n",
    "\n",
    "    print(\"\\nSpectral clustering:\")\n",
    "    print(\"Silhouette score:\", spectral_silhouette)\n",
    "    print(\"Calinski-Harabasz score:\", spectral_calinski)\n",
    "    print(\"Davies-Bouldin score:\", spectral_davies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate_clustering(data =scaled_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "According the above result Silhouette score of 0.1518645697574852 in K-means clustering indicates a moderate level of cluster quality. The score is positive, which means that the clusters are somewhat dense and reasonably separated. However, the score is not very high, suggesting that there might be some overlap between clusters.On the other hand, Spectral clustering has a Silhouette score of -0.09991550972213499, which is closer to -1. This indicates a lower quality of clustering. The negative score implies that the samples are incorrectly assigned to clusters or that the clusters are not well-separated. It suggests that Spectral clustering may not be an optimal choice for this particular dataset.\n",
    "\n",
    "Calinski-Harabasz score of 19.04378434496566 for K-means clustering suggests reasonable cluster separations in the data however score of 0.9641522971242168 for spectral clustering indicates relatively low separation between clusters.\n",
    "\n",
    "The score of 2.123765659530001 for K-means clustering suggests some overlap or similarity between clusters, but not excessively so. while, the score of 7.747962068068746 for spectral clustering indicates that the spectral clustering algorithm may not have achieved as clear separation between the clusters, with more overlap or similarity between them.\n",
    "\n",
    "In summary, based on the evaluation scores, K-means clustering performs better than spectral clustering in terms of silhouette score, Calinski-Harabasz score, and Davies-Bouldin score. K-means clustering shows relatively better separation and distinctiveness between the clusters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conclusion\n",
    "Based on the above result i choose k_mean clustering and t-sne dimention reduction and viulization the best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
